{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up env\n",
    "import pandas as pd\n",
    "import seaborn as \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SET DE ENTRENAMIENTO:** Conjunto de valores (x,y) para entrenar el algoritmo. <br>\n",
    "   \n",
    "<br>**HIPÓTESIS:** Intenta predecir valores a partir de una función lineal \n",
    "$$\\theta_0+\\theta_1\\cdot x=h(x)$$\n",
    "*Donde $\\theta_k$ son los parámetros de la función.\n",
    "\n",
    "<br>**FUNCIÓN COSTO (J):** Encontrar $\\theta_0,\\theta_1[,...,\\theta_n]$ para minimizar esta función. (Error cuadrático)\n",
    "$$\\min_{\\theta_n}\\frac{1}{2m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)^2}=\\min_{\\theta_n}J(\\theta)$$\n",
    "*Donde $(x^{(i)},y^{(i)})$ es la i-ésima pareja del set de entrenamiento.\n",
    "\n",
    "<br>**ALGORITMO - DESCENSO POR GRADIENTE:** <br>\n",
    "*(para dos parámetros)*<br>\n",
    ">1. Iniciar con *algún* $\\theta_0,\\theta_1$ <br>\n",
    ">2. Ir cambiando $\\theta_0,\\theta_1$ simultaneamente para reducir $J(\\theta_0,\\theta_1)$ hasta encontrar su mínimo<br>\n",
    "\n",
    "$$\n",
    "\\textrm{repetir hasta que }\\Delta_{\\theta_j}\\lt\\textrm{M} \\\\\n",
    "\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta_0,\\theta_1) \\\\\n",
    "\\textrm{Para  } j=0,1\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $M$ es un umbral definido\n",
    "- $\\alpha$ es la *tasa de aprendizaje* del algoritmo (qué tan rápido cambian los valores de $\\theta_j$)\n",
    "\n",
    "\n",
    "<br>**DESCENSO POR GRADIENTE PARA REGRESIÓN LINEAL:**<br>\n",
    "*(para dos parámetros)*<br>\n",
    "Debido a naturaleza convexa de la función costo, descenso por gradiente siempre encuentra el mínimo global de la función. Entonces este algoritmo puede ser utilizado para minimizarla. <br><br>\n",
    "Sabiendo que: \n",
    "$$\n",
    "h(x)=\\theta_0+\\theta_1\\cdot x \\\\\n",
    "J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)^2}\\\\\n",
    "\\theta_0:=\\theta_0-\\alpha\\frac{\\partial}{\\partial{\\theta_0}}J(\\theta_0,\\theta_1)\\\\\n",
    "\\theta_1:=\\theta_1-\\alpha\\frac{\\partial}{\\partial{\\theta_1}}J(\\theta_0,\\theta_1)\\\\\n",
    "$$\n",
    "Hallando derivadas:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta_0,\\theta_1)= \\frac{\\partial}{\\partial{\\theta_j}}\\frac{1}{2m}\\sum_{i=1}^m{\\left(\\theta_0+\\theta_1\\cdot x^{(i)}-y^{(i)}\\right)^2} \\\\\n",
    "\\frac{\\partial}{\\partial{\\theta_0}}J(\\theta_0,\\theta_1)= \\frac{1}{m}\\sum_{i=1}^m{\\left(\\theta_0+\\theta_1\\cdot x^{(i)}-y^{(i)}\\right)}=2J(\\theta_0,\\theta_1) \\\\\n",
    "\\frac{\\partial}{\\partial{\\theta_1}}J(\\theta_0,\\theta_1)= \\frac{1}{m}\\sum_{i=1}^m{\\left(\\theta_0+\\theta_1\\cdot x^{(i)}-y^{(i)}\\right)}\\cdot x^{(i)}=2J(\\theta_0,\\theta_1)\\cdot x^{(i)}\n",
    "$$\n",
    "Entonces:\n",
    "$$\n",
    "\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)}\\\\\n",
    "\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)}\\cdot x^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
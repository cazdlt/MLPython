{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning\n",
    "\n",
    "[Stanford CS234](https://web.stanford.edu/class/cs234/schedule.html)\n",
    "\n",
    "[Videos](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=2&t=0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "- ¿cómo puede un \"agente inteligente\" tomar las decisiones correctas?\n",
    "- Secuencias de decisiones\n",
    "- ¿Qué es una decisión **correcta**?\n",
    "- ¿Cómo el agente aprende a tomar decisiones?\n",
    "- Aplicaciones\n",
    "    - Robótica\n",
    "    - Videojuegos\n",
    "        - Juegos educacionales\n",
    "    - Salud\n",
    "    - Problemas de optimización\n",
    "- Implica\n",
    "    - Optimizar\n",
    "        - ¿Cómo tomar *buenas* decisiones?\n",
    "        - ¿Buenas estrategias?\n",
    "    - Consecuencias\n",
    "        - Las decisiones que tomo ahora tienen consecuencas en mi contexto en el futuro.\n",
    "        - Corto, mediano y largo plazo.\n",
    "    - Exploración\n",
    "        - ¿cómo conozco el mundo?\n",
    "        - Solo se aprende de las acciones hechas y decisiones tomadas\n",
    "        - El agente debe aprender por sí mismo\n",
    "    - Generalizar\n",
    "        - El modelo del mundo debe ser reutilizable \n",
    "        - Aprender directamente de la información\n",
    "- Otros tipos de aprendizaje \n",
    "    - Aprendizaje por imitación\n",
    "        - Aprendo a partir de ejemplos hechos por otros\n",
    "    - Aprendizaje de planeación (AI)\n",
    "    - Supervisado, no Supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toma secuencial de decisiones\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/RL1.png\">\n",
    "</p>\n",
    "\n",
    "- Interacción entre el agente y el ambiente.\n",
    "\n",
    "- Objetivo (del agente): Seleccionar acciones que maximicen la recompensa esperada a futuro.\n",
    "    - Comportamiento estratégico\n",
    "    - Puede requerir balancear entre la recompensa a corto (inmediata), mediano y largo plazo.\n",
    "\n",
    "- El desempeño del agente depende principalmente de qué acciones puede tomar y qué recompensa debe conseguir.\n",
    "- *Reward Hacking*\n",
    "  - Buscar recompensas correctas para cumplir el objetivo de la problemática.\n",
    "\n",
    "- Acciones en tiempo discreto\n",
    "  - En cada paso del tiempo:\n",
    "    - El agente emite una acción $a$\n",
    "    - El ambiente recibe la acción y emite una observación y una recompensa\n",
    "    - El agente recibe la observación del ambiente y la recompensa\n",
    "\n",
    "- El ambiente es una *representación* del mundo en donde vive el agente.\n",
    "  - No todo el ambiente es observable.\n",
    "  - Buena generalización del sistema depende de qué tan confiable es el ambiente.\n",
    "\n",
    "- Suposición de Markov:\n",
    "  - Las decisiones que se toman dependen únicamente de la observación actual, no de observaciones pasadas.\n",
    "  - Estado del mundo: observación actual.\n",
    "  - Puede ser utilizada en cualquier sistema si se toma la historia completa del sistema como el estado actual.\n",
    "  - En la práctica, generalmente la última observación es suficiente.\n",
    "\n",
    "- El estado del agente puede ser diferente al estado del ambiente.\n",
    "  - El agente puede construir su estado a partir de agregar, asumir, o transformar sus observaciones.  \n",
    "    - Representaición interna: Estado del mundo $\\neq$ Estado del agente\n",
    "  - El agente puede tener observabilidad parcial del mundo.\n",
    "\n",
    "- Las acciones del agente pueden no tener efecto sobre su ambiente\n",
    "  - Recompensa inmediata\n",
    "\n",
    "- Retos:\n",
    "    - El agente no conoce cómo funciona el mundo, solo su observación o una representación de él\n",
    "    - Se debe aprender cómo funciona el mundo a partir de su experiencia\n",
    "        - De manera implícita o explícita\n",
    "    - El agente debe aprender su propia política de acciones para obtener altas recompensas\n",
    "        - El agente aprende directamente al tomar varias acciones y \"ver qué pasa\"\n",
    "        - Balance exploración/explotación\n",
    "            - Exploración: Buscar nuevas experiencias que puedan tener altas recompensas a futuro\n",
    "            - Explotación: Utilizar las acciones ya aprendidas y que puedan tener altas recompensas según su experiencia\n",
    "            - Si tengo tiempo *infinito* para aprender: acción no depende de cuánto tiempo voy o cuánto tiempo me queda.\n",
    "            - Si tengo tiempo limitado (horizonte finito): acción depende de en qué tiempo voy. (ej. Si estoy de viaje en San Francisco, el último día no voy a explorar porque no puedo ganar nada a futuro)\n",
    "            - ¿Qué pasa si tengo un horizonte indefinido?: Hallar probabilidades para los estados en los que la situación terminaría (*sink states*).\n",
    "\n",
    "## Componentes de un agente\n",
    "- Política ($\\pi(s)$):\n",
    "  - ¿cómo el agente escoge sus acciones?\n",
    "    - Depende del estado actual\n",
    "    - Distribución (para procesos estocásticos)\n",
    "    - Función (para procesos determinísticos)\n",
    "- Modelo\n",
    "    - Modelo del entorno\n",
    "        - Representación que el agente tiene de su universo\n",
    "        - Cómo el entorno cambia según las acciones que toma el agente\n",
    "    - Modelo de la recompensa\n",
    "        - Qué recompensa obtendrá el agente tras efectuar cierta acción\n",
    "- Función de valor:\n",
    "    - Define qué recompensas va a obtener el agente se encuentra en cierto estado y sigue cierta política\n",
    "    - *Discounted Sum Of Rewards*\n",
    "        - Calcula la suma de recompensas según el componente $\\gamma$, que indica el valor de las recompensas futuras con respecto a la recompensa inmediata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesos de Markov\n",
    "- El agente solo se concentra en el estado actual del entorno\n",
    "    - No toma decisiones basadas en su historia\n",
    "    - \"El estado actual es suficiente para tomar decisiones correctas\"\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/rl3.png\">\n",
    "</p>\n",
    "## Cadenas de Markov\n",
    "- Secuencia de estados aleatorios\n",
    "- Conjunto finito de estados\n",
    "- No hay políticas ni recompensas\n",
    "    - El cambio de estado no depende de una acción, únicamente del estado actual.\n",
    "- La distribución de probabilidades de cambio de estado siempre converge a un estado estable.\n",
    "- Es posible representarlas con una matriz de transición entre estados $P$, con cierta *distribución de estado estable* (las probabilidades de cambiar a otro estado estando en el estado actual)\n",
    "    - Donde, $p(s_{i+1})=s_i*P$\n",
    "        - $P$ es una matriz $nxn$ donde $n$ representa el número total de estados\n",
    "        - $p(s_{i+1})$ indica la probabilidad de que estando en el estado $s_i$, se pase al estado $s_{i+1}$\n",
    "\n",
    "## Proceso de recompensa de Markov\n",
    "- Cadena de Markov con recompensas\n",
    "- No hay acciones\n",
    "- Horizonte: Número de pasos de tiempo en un *episodio*.\n",
    "    - Puede ser infinito\n",
    "- Valor\n",
    "    - Para un factor de descuento $\\gamma\\in [0,1]$, en un instante de tiempo $t$ se define el valor de retorno $G$ como:\n",
    "    $$\n",
    "    G_t=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\gamma^3 r_{t+3}+...\n",
    "    $$\n",
    "    - Equivalente a la recompensa del estado actual, sumada a valores disminuidos de las recompensas a futuro.\n",
    "    - Para procesos estocásticos, se define el valor de un estado $V(s)$ como la distribución de los posibles valores de retorno a partir de ese estado.\n",
    "    $$\n",
    "    V(s=s_t)=E[G_t]\n",
    "    $$\n",
    "    - Es posible computar el valor de un estado mediante\n",
    "        - Simulación (Montecarlo)\n",
    "        - Ecuación de Bellman\n",
    "        - Programación dinámica (Optimización)\n",
    "\n",
    "## Procesos de decisión de Markov\n",
    "- Proceso de recompensa de Markov con acciones\n",
    "- La política indica qué acción tomar mientras se está en un estado: $\\pi(a|s)$\n",
    "    - Puede ser determinística o estocástica\n",
    "- Si se define una política para el proceso de decisión, este puede ser modelado como un proceso de recompensa\n",
    "- El número de políticas posibles dentro de un proceso de decisión es: $|A|^{|S|}$\n",
    "    - Donde $|A|$ es el número de acciones posibles y $|S|$ es el número de estados del proceso.\n",
    "    - Asumiendo que todas las acciones son legales para todos los estados.\n",
    "    - Ej. para 2 estados ($s_1$,$s_2$) y dos acciones posibles ($a_1$,$a_2$), existen las siguientes políticas posibles:\n",
    "    \n",
    "<center>\n",
    "\n",
    "| $a_1$ | $a_1$ |\n",
    "|-------|-------|\n",
    "| $a_1$ | $a_2$ |\n",
    "| $a_2$ | $a_1$ |\n",
    "| $a_2$ | $a_2$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "### Control    \n",
    "- Función de valor de un estado:\n",
    "    - Se sigue una política $\\pi$ y se calcula el valor del estado\n",
    "    - Representada como\n",
    "    $$\n",
    "    V^\\pi(s)\n",
    "    $$\n",
    "- Valor de estado-acción\n",
    "    - Se toma una acción $a$ y se toma la recompensa inmediata, luego se sigue la política $\\pi$\n",
    "    - Representado como\n",
    "    $$\n",
    "    Q^\\pi(s,a)\n",
    "    $$\n",
    "- Para un proceso de decisión de Markov con horizonte infinito, la política óptima es determinística, pero no única.\n",
    "    - Hay un único valor óptimo para cada estado\n",
    "    - Este valor óptimo puede ser alcanzable por una o más políticas. \n",
    "- ¿Cómo hallar una política óptima? ***Policy Search***\n",
    "    - *Policy Iteration*\n",
    "        - Algoritmo iterativo\n",
    "        - Garantiza hallar una política óptima global\n",
    "    - *Value Iteration*\n",
    "    - Técnicas basadas en gradiente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de políticas\n",
    "\n",
    "Si se fija una política, ¿qué tan buena es?\n",
    "\n",
    "- Siendo $\\hat\\theta$ los parámetros hallados que estiman los parámetros reales (\\theta) de una distribución y E[x] un valor esperado (media, mediana..)\n",
    "  - Bias: $E[\\hat\\theta] -\\theta$\n",
    "    - ¿cómo se calcula si no se conoce $\\theta$?\n",
    "  - Varianza: $E[(\\hat\\theta-E[\\hat\\theta])^2]$\n",
    "  - MSE: $Var(\\hat\\theta)+Bias(\\hat\\theta)^2$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/rl2.png\">\n",
    "</p>\n",
    "\n",
    "## Modelo conocido\n",
    "- Programación dinámica\n",
    "- Ecuaciones de Bellman\n",
    "- Se asume criterio de Markov\n",
    "\n",
    "## Modelo desconocido\n",
    "### Montecarlo\n",
    "- El valor de la política es igual al promedio de los retornos esperados si se sigue la política.\n",
    "- Simular el sistema siguiente una política múltiples veces y hallar el promedio.\n",
    "- Cada episodio del sistema debe finalizar.\n",
    "- Alta varianza\n",
    "  - Requiere grandes cantidades de datos\n",
    "- El retorno esperado aún es $G_t$\n",
    "- No asume Markov\n",
    "- Algoritmos:\n",
    "  - *First-visit Montecarlo for Policy Evaluation*\n",
    "  - *Every-visit Montecarlo for Policy Evaluation*\n",
    "  - *Incremental Montecarlo for Policy Evaluation*\n",
    "    - Útil para dominios no estacionarios (cuando el modelo del mundo cambia con el tiempo)\n",
    "### Temporal Difference (TD) Learning\n",
    "- *Sutton y Barto (2017)*\n",
    "- Combinación entre Montecarlo y programación dinámica\n",
    "  - Montecarlo: Siempre toma la verdadera muestra de la recompensa (desde la simulación)\n",
    "  - Programación dinámica: Toma el *valor esperado* de la recompensa (*bootstraping*).\n",
    "- Funciona para sistemas con episodios o con horizonte infinito\n",
    "- No requiere modelo del mundo ni de recompensa\n",
    "- Es Incremental: se actualiza después de cada cambio de estado del agente\n",
    "- Alto bias y varianza pero rápido y flexible\n",
    "- Markov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas\n",
    "- *Machine Teaching*\n",
    "  - Cooperación entre agentes\n",
    "  - Los agentes saben que están cooperando entre sí."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
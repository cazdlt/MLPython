{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "- ¿cómo puede un \"agente inteligente\" tomar las decisiones correctas?\n",
    "- Secuencias de decisiones\n",
    "- ¿Qué es una decisión **correcta**?\n",
    "- ¿Cómo el agente aprende a tomar decisiones?\n",
    "- Aplicaciones\n",
    "    - Robótica\n",
    "    - Videojuegos\n",
    "        - Juegos educacionales\n",
    "    - Salud\n",
    "    - Problemas de optimización\n",
    "- Implica\n",
    "    - Optimizar\n",
    "        - ¿Cómo tomar *buenas* decisiones?\n",
    "        - ¿Buenas estrategias?\n",
    "    - Consecuencias\n",
    "        - Las decisiones que tomo ahora tienen consecuencas en mi contexto en el futuro.\n",
    "        - Corto, mediano y largo plazo.\n",
    "    - Exploración\n",
    "        - ¿cómo conozco el mundo?\n",
    "        - Solo se aprende de las acciones hechas y decisiones tomadas\n",
    "        - El agente debe aprender por sí mismo\n",
    "    - Generalizar\n",
    "        - El modelo del mundo debe ser reutilizable \n",
    "        - Aprender directamente de la información\n",
    "- Otros tipos de aprendizaje \n",
    "    - Aprendizaje por imitación\n",
    "        - Aprendo a partir de ejemplos hechos por otros\n",
    "    - Aprendizaje de planeación (AI)\n",
    "    - Supervisado, no Supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toma secuencial de decisiones\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/RL1.png\">\n",
    "</p>\n",
    "\n",
    "- Interacción entre el agente y el ambiente.\n",
    "\n",
    "- Objetivo (del agente): Seleccionar acciones que maximicen la recompensa esperada a futuro.\n",
    "    - Comportamiento estratégico\n",
    "    - Puede requerir balancear entre la recompensa a corto (inmediata), mediano y largo plazo.\n",
    "\n",
    "- El desempeño del agente depende principalmente de qué acciones puede tomar y qué recompensa debe conseguir.\n",
    "- *Reward Hacking*\n",
    "  - Buscar recompensas correctas para cumplir el objetivo de la problemática.\n",
    "\n",
    "- Acciones en tiempo discreto\n",
    "  - En cada paso del tiempo:\n",
    "    - El agente emite una acción $a$\n",
    "    - El ambiente recibe la acción y emite una observación y una recompensa\n",
    "    - El agente recibe la observación del ambiente y la recompensa\n",
    "\n",
    "- El ambiente es una *representación* del mundo en donde vive el agente.\n",
    "  - No todo el ambiente es observable.\n",
    "  - Buena generalización del sistema depende de qué tan confiable es el ambiente.\n",
    "\n",
    "- Suposición de Markov:\n",
    "  - Las decisiones que se toman dependen únicamente de la observación actual, no de observaciones pasadas.\n",
    "  - Estado del mundo: observación actual.\n",
    "  - Puede ser utilizada en cualquier sistema si se toma la historia completa del sistema como el estado actual.\n",
    "  - En la práctica, generalmente la última observación es suficiente.\n",
    "\n",
    "- El estado del agente puede ser diferente al estado del ambiente.\n",
    "  - El agente puede construir su estado a partir de agregar, asumir, o transformar sus observaciones.  \n",
    "    - Representaición interna: Estado del mundo $\\neq$ Estado del agente\n",
    "  - El agente puede tener observabilidad parcial del mundo.\n",
    "\n",
    "- Las acciones del agente pueden no tener efecto sobre su ambiente\n",
    "  - Recompensa inmediata\n",
    "\n",
    "- Retos:\n",
    "    - El agente no conoce cómo funciona el mundo, solo su observación o una representación de él\n",
    "    - Se debe aprender cómo funciona el mundo a partir de su experiencia\n",
    "        - De manera implícita o explícita\n",
    "    - El agente debe aprender su propia política de acciones para obtener altas recompensas\n",
    "        - El agente aprende directamente al tomar varias acciones y \"ver qué pasa\"\n",
    "        - Balance exploración/explotación\n",
    "            - Exploración: Buscar nuevas experiencias que puedan tener altas recompensas a futuro\n",
    "            - Explotación: Utilizar las acciones ya aprendidas y que puedan tener altas recompensas según su experiencia\n",
    "            - Si tengo tiempo *infinito* para aprender: acción no depende de cuánto tiempo voy o cuánto tiempo me queda.\n",
    "            - Si tengo tiempo limitado (horizonte finito): acción depende de en qué tiempo voy. (ej. Si estoy de viaje en San Francisco, el último día no voy a explorar porque no puedo ganar nada a futuro)\n",
    "            - ¿Qué pasa si tengo un horizonte indefinido?: Hallar probabilidades para los estados en los que la situación terminaría (*sink states*).\n",
    "\n",
    "## Componentes de un agente\n",
    "- Política ($\\pi(s)$):\n",
    "  - ¿cómo el agente escoge sus acciones?\n",
    "    - Depende del estado actual\n",
    "    - Distribución (para procesos estocásticos)\n",
    "    - Función (para procesos determinísticos)\n",
    "- Modelo\n",
    "    - Modelo del entorno\n",
    "        - Representación que el agente tiene de su universo\n",
    "        - Cómo el entorno cambia según las acciones que toma el agente\n",
    "    - Modelo de la recompensa\n",
    "        - Qué recompensa obtendrá el agente tras efectuar cierta acción\n",
    "- Función de valor:\n",
    "    - Define qué recompensas va a obtener el agente se encuentra en cierto estado y sigue cierta política\n",
    "    - *Discounted Sum Of Rewards*\n",
    "        - Calcula la suma de recompensas según el componente $\\gamma$, que indica el valor de las recompensas futuras con respecto a la recompensa inmediata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesos de Markov\n",
    "- El agente solo se concentra en el estado actual del entorno\n",
    "    - No toma decisiones basadas en su historia\n",
    "    - \"El estado actual es suficiente para tomar decisiones correctas\"\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/rl3.png\">\n",
    "</p>\n",
    "## Cadenas de Markov\n",
    "- Secuencia de estados aleatorios\n",
    "- Conjunto finito de estados\n",
    "- No hay políticas ni recompensas\n",
    "    - El cambio de estado no depende de una acción, únicamente del estado actual.\n",
    "- La distribución de probabilidades de cambio de estado siempre converge a un estado estable.\n",
    "- Es posible representarlas con una matriz de transición entre estados $P$, con cierta *distribución de estado estable* (las probabilidades de cambiar a otro estado estando en el estado actual)\n",
    "    - Donde, $p(s_{i+1})=s_i*P$\n",
    "        - $P$ es una matriz $nxn$ donde $n$ representa el número total de estados\n",
    "        - $p(s_{i+1})$ indica la probabilidad de que estando en el estado $s_i$, se pase al estado $s_{i+1}$\n",
    "\n",
    "## Proceso de recompensa de Markov\n",
    "- Cadena de Markov con recompensas\n",
    "- No hay acciones\n",
    "- Horizonte: Número de pasos de tiempo en un *episodio*.\n",
    "    - Puede ser infinito\n",
    "- Valor\n",
    "    - Para un factor de descuento $\\gamma\\in [0,1]$, en un instante de tiempo $t$ se define el valor de retorno $G$ como:\n",
    "    $$\n",
    "    G_t=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\gamma^3 r_{t+3}+...\n",
    "    $$\n",
    "    - Equivalente a la recompensa del estado actual, sumada a valores disminuidos de las recompensas a futuro.\n",
    "    - Para procesos estocásticos, se define el valor de un estado $V(s)$ como la distribución de los posibles valores de retorno a partir de ese estado.\n",
    "    $$\n",
    "    V(s=s_t)=E[G_t]\n",
    "    $$\n",
    "    - Es posible computar el valor de un estado mediante\n",
    "        - Simulación (Montecarlo)\n",
    "        - Ecuación de Bellman\n",
    "        - Programación dinámica (Optimización)\n",
    "\n",
    "## Procesos de decisión de Markov\n",
    "- Proceso de recompensa de Markov con acciones\n",
    "- La política indica qué acción tomar mientras se está en un estado: $\\pi(a|s)$\n",
    "    - Puede ser determinística o estocástica\n",
    "- Si se define una política para el proceso de decisión, este puede ser modelado como un proceso de recompensa\n",
    "- El número de políticas posibles dentro de un proceso de decisión es: $|A|^{|S|}$\n",
    "    - Donde $|A|$ es el número de acciones posibles y $|S|$ es el número de estados del proceso.\n",
    "    - Asumiendo que todas las acciones son legales para todos los estados.\n",
    "    - Ej. para 2 estados ($s_1$,$s_2$) y dos acciones posibles ($a_1$,$a_2$), existen las siguientes políticas posibles:\n",
    "    \n",
    "<center>\n",
    "\n",
    "| $a_1$ | $a_1$ |\n",
    "|-------|-------|\n",
    "| $a_1$ | $a_2$ |\n",
    "| $a_2$ | $a_1$ |\n",
    "| $a_2$ | $a_2$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "### Control    \n",
    "- Función de valor de un estado:\n",
    "    - Se sigue una política $\\pi$ y se calcula el valor del estado\n",
    "    - Representada como\n",
    "    $$\n",
    "    V^\\pi(s)\n",
    "    $$\n",
    "- Valor de estado-acción\n",
    "    - Se toma una acción $a$ y se toma la recompensa inmediata, luego se sigue la política $\\pi$\n",
    "    - Representado como\n",
    "    $$\n",
    "    Q^\\pi(s,a)\n",
    "    $$\n",
    "- Para un proceso de decisión de Markov con horizonte infinito, la política óptima es determinística, pero no única.\n",
    "    - Hay un único valor óptimo para cada estado\n",
    "    - Este valor óptimo puede ser alcanzable por una o más políticas. \n",
    "- ¿Cómo hallar una política óptima? ***Policy Search***\n",
    "    - *Policy Iteration*\n",
    "        - Algoritmo iterativo\n",
    "        - Garantiza hallar una política óptima global\n",
    "    - *Value Iteration*\n",
    "    - Técnicas basadas en gradiente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de políticas\n",
    "\n",
    "Si se fija una política, ¿qué tan buena es?\n",
    "\n",
    "- Siendo $\\hat\\theta$ los parámetros hallados que estiman los parámetros reales (\\theta) de una distribución y E[x] un valor esperado (media, mediana..)\n",
    "  - Bias: $E[\\hat\\theta] -\\theta$\n",
    "    - ¿cómo se calcula si no se conoce $\\theta$?\n",
    "  - Varianza: $E[(\\hat\\theta-E[\\hat\\theta])^2]$\n",
    "  - MSE: $Var(\\hat\\theta)+Bias(\\hat\\theta)^2$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/rl2.png\">\n",
    "</p>\n",
    "\n",
    "## Modelo conocido\n",
    "- Programación dinámica\n",
    "- Ecuaciones de Bellman\n",
    "- Se asume criterio de Markov\n",
    "\n",
    "## Modelo desconocido\n",
    "### Montecarlo\n",
    "- El valor de la política es igual al promedio de los retornos esperados si se sigue la política.\n",
    "- Simular el sistema siguiente una política múltiples veces y hallar el promedio.\n",
    "- Cada episodio del sistema debe finalizar.\n",
    "- Alta varianza\n",
    "  - Requiere grandes cantidades de datos\n",
    "- El retorno esperado aún es $G_t$\n",
    "- No asume Markov\n",
    "- Algoritmos:\n",
    "  - *First-visit Montecarlo for Policy Evaluation*\n",
    "  - *Every-visit Montecarlo for Policy Evaluation*\n",
    "  - *Incremental Montecarlo for Policy Evaluation*\n",
    "    - Útil para dominios no estacionarios (cuando el modelo del mundo cambia con el tiempo)\n",
    "### Temporal Difference (TD) Learning\n",
    "- *Sutton y Barto (2017)*\n",
    "- Combinación entre Montecarlo y programación dinámica\n",
    "  - Montecarlo: Siempre toma la verdadera muestra de la recompensa (desde la simulación)\n",
    "  - Programación dinámica: Toma el *valor esperado* de la recompensa (*bootstraping*).\n",
    "- Toma la recompensa inmediata real (siguiendo la política | Montecarlo) y el valor esperado de la siguiente recompensa (Bellmann).\n",
    "- Funciona para sistemas con episodios o con horizonte infinito\n",
    "- No requiere modelo del mundo ni de recompensa\n",
    "- Es Incremental: se actualiza después de cada cambio de estado del agente\n",
    "- Alto bias y varianza pero rápido y flexible\n",
    "- Markov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control\n",
    "- ¿Cómo toa el agente las decisiones que mazimizan su recompensa a corto, mediano o largo plazo?\n",
    "## Sin modelo\n",
    "- El agente no posee un modelo explícito del mundo\n",
    "    - Dinámicas del mundo\n",
    "    - Recompensa\n",
    "- El agente aprende únicamente a partir de experiencia\n",
    "- Se conoce el espacio de acciones\n",
    "- Requiere\n",
    "    - Exploración\n",
    "    - Optimización\n",
    "    - Evaluación de consecuencias (con retraso)\n",
    "- *Policy iteration* (para procesos de decisión de Markov):\n",
    "    - Inicializar la política $\\pi$\n",
    "    - Repetir hasta convergencia:\n",
    "        - Evaluar la política: $Q^\\pi$    \n",
    "            - Montecarlo para Q Learning\n",
    "        - Mejorar la política (policy improvement):\n",
    "            - Dada una evaluación de la política $\\pi$: $Q^\\pi(s,1)\\quad\\forall_{s,a}$, se actualiza $\\pi$ como:\n",
    "            $$\n",
    "            \\pi_{i+1}=arg\\max_aQ^\\pi(s,a)\n",
    "            $$\n",
    "            - Exploración:\n",
    "                - Para cada iteración existe una probabilidad $\\epsilon$ de escoger una acción aleatoria  $a$ (que puede ser diferente a aquella que maximiza Q)\n",
    "- Montecarlo para control\n",
    "- TD Learning para control  \n",
    "    - Algoritmo SARSA      \n",
    "    - **Q Learning**\n",
    "    - Double Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aproximación de funciones\n",
    "- Se aproxima la función de valor ($V(s)$ o $Q(s,a)$)\n",
    "    - A partir de un estado $s$ y una función de aproximación, se calcula el valor:\n",
    "        - $\\hat V(s;w)$\n",
    "        - $\\hat Q(s,a;w)$\n",
    "        - Donde $w$ es el vector de parámetros de la función de aproximación.\n",
    "- La función de aproximación puede ser:\n",
    "    - Redes neuronales\n",
    "    - Árboles de decisión\n",
    "    - Polinomios\n",
    "    - ...\n",
    "- Necesario para problemas con gran cantidad de estados y/o acciones.\n",
    "- Se requiere generalizacióna partir de la experiencia anterior\n",
    "## Aproximación por funciones lineales\n",
    "- Para una función diferenciable $J(\\vec{w})$\n",
    "    - Donde $\\vec{w}$ es el vector de parámetros de $J$\n",
    "    - Se utiliza *gradient descent* para encontrar los parámetros $\\vec w$ que minimicen (o maximicen) $J$\n",
    "    - Para cada iteración $k$ del algoritmo de descenso por gradiente:\n",
    "        - $\\vec w_{k+1}=\\vec w_k-\\alpha\\nabla J(\\vec w)$\n",
    "            - Donde $\\alpha$ es la tasa de aprendizaje\n",
    "            - $\\nabla J(\\vec w)$ son las derivadas parciales de $J$ con respecto a cada parámetro de $\\vec w$  \n",
    "    - Generalmente cada $k$ es un estado visitado (*stochastic gradient descent*)\n",
    "### Para evaluación de políticas (predicción)\n",
    "- Montecarlo para evaluación de políticas usando aproximación lineal de funciones\n",
    "    - Cada estado $s$ es representado por un vector de características $X(s)$\n",
    "    - $X(s)$ puede ser o la observación del agente o una represetanción interna de esta observación (características de alto nivel)\n",
    "        - Historia de estados anteriores, por ejemplo\n",
    "    - $V^\\pi(s;w)=\\sum_j^n x_j(s)^T\\cdot w_j$\n",
    "        - Donde $n$ es el número de características\n",
    "    - $J(\\vec w)=||V^\\pi(s)-V^\\pi(s;\\vec w)||^2$ (MSE)\n",
    "        - (Dado el valor real de $V^pi(s)$, se convierte en aprendizaje supervisado\n",
    "        - Para hallar $V^\\pi$ se usa Montecarlo (el cual da una precisión \"ruidosa\" -*biased*- pero exacta)\n",
    "- TD Learning para evaluación de políticas usando aproximación lineal de funciones\n",
    "### Control\n",
    "- El vector de características $X$ ahora es representado por parejas (estado,acción)\n",
    "- La combinación de:\n",
    "    - Aproximación de funciones\n",
    "    - Bootstraping (TD)\n",
    "    - Entrenamiento fuera de política (exploración $\\epsilon $)\n",
    "        - Off-policy\n",
    "## Aproximación por redes neuronales\n",
    "- DeepMind\n",
    "- Deep Q Learning / Double DQN\n",
    "    - Dueling DQN\n",
    "- Para mejorar estabilidad:\n",
    "    - Experience Replay\n",
    "        - Prioritized replay\n",
    "    - Fixed Q-Targets\n",
    "## Imitation learning\n",
    "- Behavioral cloning\n",
    "    - Problema de aprendizaje supervisado\n",
    "    - Aprende totalmente de un \"experto\"\n",
    "- Inverse RL\n",
    "    - Aprende la función de recompensa a partir de demostraciones\n",
    "    - Nunca hay una función de recompensa óptima si solo se obtiene a partir de ejemplos\n",
    "    - GAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas\n",
    "- *Machine Teaching*\n",
    "  - Cooperación entre agentes\n",
    "  - Los agentes saben que están cooperando entre sí.\n",
    "- Meta-computation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
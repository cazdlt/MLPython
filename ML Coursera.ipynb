{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANA 1\n",
    "**SET DE ENTRENAMIENTO:** Conjunto de valores (x,y) para entrenar el algoritmo. <br>\n",
    "   \n",
    "<br>**HIPÓTESIS:** Intenta predecir valores a partir de una función lineal \n",
    "$$\\theta_0+\\theta_1\\cdot x=h(x)$$\n",
    "*Donde $\\theta_k$ son los parámetros de la función.\n",
    "\n",
    "<br>**FUNCIÓN COSTO (J):** Encontrar $\\theta_0,\\theta_1[,...,\\theta_n]$ para minimizar esta función. (Error cuadrático)\n",
    "$$\\min_{\\theta_n}\\frac{1}{2m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)^2}=\\min_{\\theta_n}J(\\theta)$$\n",
    "*Donde $(x^{(i)},y^{(i)})$ es la i-ésima pareja del set de entrenamiento.\n",
    "\n",
    "<br>**ALGORITMO - DESCENSO POR GRADIENTE:** <br>\n",
    "*(para dos parámetros)*<br>\n",
    ">1. Iniciar con *algún* $\\theta_0,\\theta_1$ <br>\n",
    ">2. Ir cambiando $\\theta_0,\\theta_1$ simultaneamente para reducir $J(\\theta_0,\\theta_1)$ hasta encontrar su mínimo<br>\n",
    "\n",
    "$$\n",
    "\\textrm{repetir hasta que }\\Delta_{\\theta_j}\\lt\\textrm{M} \\\\\n",
    "\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta_0,\\theta_1) \\\\\n",
    "\\textrm{Para  } j=0,1\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $M$ es un umbral definido\n",
    "- $\\alpha$ es la *tasa de aprendizaje* del algoritmo (qué tan rápido cambian los valores de $\\theta_j$)\n",
    "\n",
    "\n",
    "<br>**DESCENSO POR GRADIENTE PARA REGRESIÓN LINEAL:**<br>\n",
    "*(para dos parámetros)*<br>\n",
    "Debido a naturaleza convexa de la función costo, descenso por gradiente siempre encuentra el mínimo global de la función. Entonces este algoritmo puede ser utilizado para minimizarla. <br><br>\n",
    "Sabiendo que: \n",
    "$$\n",
    "h(x)=\\theta_0+\\theta_1\\cdot x \\\\\n",
    "J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)^2}\\\\\n",
    "\\theta_0:=\\theta_0-\\alpha\\frac{\\partial}{\\partial{\\theta_0}}J(\\theta_0,\\theta_1)\\\\\n",
    "\\theta_1:=\\theta_1-\\alpha\\frac{\\partial}{\\partial{\\theta_1}}J(\\theta_0,\\theta_1)\\\\\n",
    "$$\n",
    "Hallando derivadas:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta_0,\\theta_1)= \\frac{\\partial}{\\partial{\\theta_j}}\\frac{1}{2m}\\sum_{i=1}^m{\\left(\\theta_0+\\theta_1\\cdot x^{(i)}-y^{(i)}\\right)^2} \\\\\n",
    "\\frac{\\partial}{\\partial{\\theta_0}}J(\\theta_0,\\theta_1)= \\frac{1}{m}\\sum_{i=1}^m{\\left(\\theta_0+\\theta_1\\cdot x^{(i)}-y^{(i)}\\right)} \\\\\n",
    "\\frac{\\partial}{\\partial{\\theta_1}}J(\\theta_0,\\theta_1)= \\frac{1}{m}\\sum_{i=1}^m{\\left(\\theta_0+\\theta_1\\cdot x^{(i)}-y^{(i)}\\right)}\\cdot x^{(i)}\n",
    "$$\n",
    "Entonces:\n",
    "$$\n",
    "\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)}\\\\\n",
    "\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum_{i=1}^m{\\left(h(x^{(i)})-y^{(i)}\\right)}\\cdot x^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver código en regresionLineal.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMANA 2\n",
    "**MÚLTIPLES CARACTERÍSTICAS:** Ahora se tienen en cuenta $n$ parámetros para cada fila.<br>\n",
    " - $x^{(i)}$ es el vector de parámetros de entrada para la i-ésima fila. <br>\n",
    " - $x_j^{(i)}$ es el valor del j-ésimo parámetro de la i-ésima fila.\n",
    "\n",
    " Ahora hipótesis es:\n",
    " $$\n",
    " h_\\theta(x)=\\theta_0\\cdot x_0+\\theta_1\\cdot x_1+\\theta_2\\cdot x_2+...+\\theta_n\\cdot x_n\n",
    " $$\n",
    " Donde $x_0=1$ <br>\n",
    " $x$ y $\\theta$ se pueden representar como:\n",
    " $$\n",
    " x=\n",
    " \\begin{bmatrix} \n",
    "    x_0  \\\\ \n",
    "    x_1  \\\\ \n",
    "    ...  \\\\\n",
    "    x_n\n",
    " \\end{bmatrix}\n",
    " $$\n",
    "y\n",
    " $$\n",
    " \\theta=\n",
    " \\begin{bmatrix} \n",
    "    \\theta_0  \\\\ \n",
    "    \\theta_1  \\\\ \n",
    "    ...  \\\\\n",
    "    \\theta_n\n",
    " \\end{bmatrix}\n",
    " \n",
    " $$\n",
    "\n",
    " Entonces:\n",
    " $$\n",
    " h(x)=\\theta^T \\cdot x\n",
    " $$\n",
    "\n",
    " **DESCENSO POR GRADIENTE PARA MÚLTIPLES CARACTERÍSTICAS:**<br>\n",
    " Función costo:\n",
    " $$\n",
    " J(\\Theta)=\\frac{1}{2m}\\cdot \\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)^2\n",
    " $$\n",
    " Algoritmo:\n",
    " $$\n",
    "\\textrm{repetir hasta que }\\Delta_{\\theta_j}\\lt\\textrm{M} \\\\\n",
    "\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial{\\theta_j}}J(\\Theta) \\\\\n",
    "\\rightarrow\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^m\\left( (h_\\theta(x^{i})-y^{(i)})\\cdot x_j^{(i)}\\right) \\\\\n",
    "\\textrm{Para todo } j=0..n\n",
    "$$\n",
    "$$\n",
    "\n",
    "**ESCALAMIENTO DE CARACTERÍSTICAS:**<br>\n",
    "Si una característica de entrada tiene valores muy diferentes a otras características de entrada o salida, es conveniente normalizar estas características a valores cercanos a $-1 < x < 1$, de la forma:\n",
    "$$\n",
    "x=\\frac{x-\\mu_x}{\\sigma_x}\n",
    "$$\n",
    "Donde, $\\mu_x$ es la media de $x$ y $\\sigma_x$ es la desviación estándar o el rango de valores de $x$ (Es decir, $x_{max}-x_{min}$)\n",
    "\n",
    "**CARACTERÍSTICAS Y REGRESIÓN POLINOMIAL:**<br>\n",
    " - Es posible unir varias características para simplificar un modelo. Por ejemplo si dos de mis características son *Largo* y *Ancho*, puede ser que la nueva característica sea $Area=Largo\\cdot Ancho$\n",
    " - Es posible convertir la regresión lineal en regresión polinomial, si simplemente escalamos las características a diferentes potencias. Por ejemplo (siendo x una característica de entrada): \n",
    " $$\n",
    " y=\\theta_0+x\\cdot \\theta_1+x^2\\cdot \\theta_2\n",
    " $$\n",
    "\n",
    " Se puede modelar de manera sencilla simplemente creando un arreglo con x^2. Este procedimiento puede ser replicado para otras funciones matemáticas, por ejemplo $\\sqrt{x}$ o $\\ln{x}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}